# 第二部分 深层神经网络

## 2.1.深度学习与深层神经网络

### 2.1.1.线性模型与线性可分问题

若，一个模型的输入和输出满足一下关系，那么这个模型就是一个线性模型：
$$
y = Σwx+b
$$
线性模型的特点：任意线性模型的组合仍然是线性模型。

**线性可分问题：**若一个问题可以用一条直线进行分类，那么线性模型就可以解决这个问题。即线性可分问题。

### 2.1.2.激活函数

**作用：**1.去线性化；2.归一化。

**三种常见的激活函数：**

<u>***Sigmoid函数：***</u>

![img](https://img-blog.csdn.net/20180125173446522)

图像如下：

![img](https://img-blog.csdn.net/20180125174343243)

导数图像：

![img](https://img-blog.csdn.net/20180125174449393)

Sigmoid 函数的三个主要缺陷：

1.**梯度消失：** 注意：Sigmoid 函数趋近 0 和 1 的时候变化率会变得平坦，也就是说，Sigmoid 的梯度趋近于 0。神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0。这些神经元叫作饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新得很慢。该问题叫作梯度消失。因此，想象一下，如果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。

2.**不以零为中心：** Sigmoid 输出不以零为中心的。

3.**计算成本高昂：** exp() 函数与其他非线性激活函数相比，计算成本高昂。

<u>***Tanh函数：***</u>

图像如下：

![img](https://img-blog.csdn.net/20180125191958415)

导数图像如下：

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180125192050985)

Tanh函数缺陷：

存在梯度消失问题，在正无穷和负无穷方向上。

<u>***ReLU函数：***</u>

函数图像:

![img](https://img-blog.csdn.net/20180125192306041)

导数图像：

![img](https://img-blog.csdn.net/20180125192353941)

优点：他不会存在饱和问题，即可以对抗梯度消失问题并且可以快速的收敛。

缺点：不以零为中心。



**代码示例：**

`a = tf.nn.relu(tf.matmul(x, w1) + bias1)`

## 2.2.损失函数

### 2.2.1.经典损失函数

分类问题和回归问题是监督学习的两大问题。对于分类问题：

我们常常使用**softmax-交叉熵**作为损失函数，其中：

交叉熵是用来衡量两个分布之间接近程度的函数。即：

给定两个概率分布p与q，交叉熵值越小说明两个分布越接近。交叉熵可以表示为：（刻画的是用q来表达概率分布p的困难程度）
$$
H(p, q)= -Σp(x)logq(x)
$$
然而神经网络的输出却不一定是一个概率分布，这意味着我们要先找到把事件类别转化为分布的办法。于是引用了softmax函数进行计算。不难得出，softmax输出为一个数组，而交叉熵的输出为一个数值。

其中，softmax-交叉熵函数的调用实例如下：

`cross_entropy = tf.nn.softmax_cross_entropy_with_logits(label = y_, logits = y)`

注意，在Tensorflow之后的版本中，将该函数更新为了：

`tf.nn.softmax_cross_entropy_with_logits_v2(`labels,  logits,  axis = None, name = None,

dim = None`)`

对于回归问题可以使用均方误差（MSE）：

![img](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D158/sign=3b8c870fafc27d1ea1263fc123d4adaf/f636afc379310a555caaeb5fbc4543a98326108a.jpg)

示例：

`mse = tf.reduce_mean(tf.square(y_ - y))`

## 2.3.神经网络的优化问题

### **2.3.1.梯度下降（优化单个参数）**

是一个一阶最优化算法，也称为最速下降法。使用梯度下降法寻找一个函数的局部最小值，必须向函数上当前点对应梯度的反方向规定步长距离点进行迭代。

参数的梯度可以通过求偏导的方式产生。同时，学习率η可以定义每次参数更新的速度，直观上可以理解为每次参数移动的幅度。

值得注意的是，梯度下降算法不能保证被优化的函数达到的是全局最优解，同时梯度下降还包括这些缺点：靠近极值时速度减慢；直线搜索可能会出现一些问题；可能会出现“之字型”下降，速度会异常的缓慢。

为了解决这些问题，可以使用每次在一个batch上进行优化的办法来解决，首先，在一个batch上优化神经网络参数并不会比单个数据慢太多；其次使用batch可以大大减小收敛所需要的迭代次数，同时可以使收敛效果更加接近梯度下降的效果。

### 2.3.2.学习率设置

学习率决定了参数每次更新的幅度。如果幅度太大，那么可能导致参数在极优值的两侧来回移动。，相反如果学习率过小，虽然能保证收敛性但是会大大降低优化的效率。

Tensorflow提供了指数衰减法来提供灵活的学习率设置方法，即：

`tf.train_exponential_decay()`

函数可以实现指数衰减学习率。它实现了以下功能来更新学习率：

`decayed_learning_rate = learning_rate * decay_rate^(global_step/decay_steps)`

其中learning_rate代表初始学习率，decay_rate为衰减系数，decay_steps为衰减速率。

例子：

```python
global_step = tf.Variable(0)`

`learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircass = True)`

`learning_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)`
```

上面这段代码设置初始学习率0.1，并且因为指定staircase=True，所以每训练10轮后学习率乘以0.96。

### 2.3.3.过拟合问题

**正则化思想：**在损失函数中加入刻画模型复杂度的指标。即假设刻画模型训练情况的损失函数为
$$
J(θ)
$$
现在我们不再直接优化它，转而优化
$$
J(θ)+λR（w）
$$
其中R(w)就是刻画模型复杂度的指标。而λ表示模型复杂损失在总损失中的比例。通常使用的刻画复杂度的函数有两种，一种是L1正则化，另一种是L2正则化。其中L1范数的特点是它会使得参数变得稀疏，并且其公式不可导而且带有L1范数的正则化会更加的复杂，而L2不存在以上问题。

以下为使用示例：

```python
weights = tf.constant([[1., -2.], [-3., 4.]])
with tf.Session() as sess:
    print(sess.run(tf.contrib.layers.l1_regularizer(.5)(weights)))
    print(sess.run(tf.contrib.layers.l2_regularizer(.5)(weights)))
```

### 2.3.4.滑动平均模型

使用滑动平均模型在很多应用中都可以一定程度的提高最终模型在测试数据上的表现。

Tensorflow中提供了以下语句来实现滑动平均模型：

`tf.train.ExponentialMovingAverage()`

在初始化时需要提供衰减率(decay)，用于控制模型的更新速度。该函数会对每一个变量生成一个影子变量，这个影子变量的初始值就是响应变量的初始值，而每次运行变量更新时，影子变量都会如下更新：

`shadow_variable = decay * shadow_variable +(1 - decay)*variable`

在实际应用中，decay一般设置为非常接近1的数，同时如果在初始化时提供了num_updata参数，那么就可以动态设置decay的值了。

